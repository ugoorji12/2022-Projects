{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/orjiugochukwu/ml-models-for-detecting-diabetes-with-99-accuracy?scriptVersionId=106036132\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# ML Models for Detecting Onset Diabetes\nBy\n\n**Ugochukwu Orji**","metadata":{"papermill":{"duration":0.057911,"end_time":"2022-01-04T06:53:05.291583","exception":false,"start_time":"2022-01-04T06:53:05.233672","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Executive Summary of Project\n\n* The Diabetes mellitus disease is fast rising to epidemic levels all over the world according to WHO reports. When it is not effectively treated, it can lead to organ failure, cardiovascular disease, other bodily functions being disrupted and then death. \n\n* Stakeholders in the health industry have been seeking machine learning (ML) tools and techniques that can assist healthcare practitioners in the early stage of diabetes diagnosis so as to halt the lethal disease's progression. Recently, clinical trials have employed ML, data mining, and big data technologies to predict diabetes diagnosis and other diseases in patients. \n\n* Exploratory data analysis on the data showed severe skewness in class distribution,  then random under-sampling technique was introduced to handle this and deployed 5 Machine Learning (ML) models including; Random Forest Classifier (RF), Decision Tree Classifier (DT), XGBoost Classifier (XGB), KNeighbors Classifier (KNN) and Logistic Regression (LR) to build the models. \n\n* The models were evaluated using the confusion matrix and classification report approach, the ROC curve and AUC score of the models were also introduced. Finally, the RF model obtained the best accuracy score of 99% followed by DT, XGB, KNN & LR models with an accuracy score of 98%, 97%, 80% & 75% respectively. ","metadata":{}},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, ParameterGrid, cross_val_score, RepeatedKFold, RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings  as ws\nws.filterwarnings(\"ignore\")","metadata":{"papermill":{"duration":1.358317,"end_time":"2022-01-04T06:53:06.801682","exception":false,"start_time":"2022-01-04T06:53:05.443365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.110179Z","iopub.execute_input":"2022-08-21T21:08:50.111477Z","iopub.status.idle":"2022-08-21T21:08:50.120236Z","shell.execute_reply.started":"2022-08-21T21:08:50.111378Z","shell.execute_reply":"2022-08-21T21:08:50.118739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load data","metadata":{}},{"cell_type":"code","source":"# load the dataframe\ndf=pd.read_csv(\"../input/diabetes-data-set/diabetes-dataset.csv\")","metadata":{"papermill":{"duration":0.056857,"end_time":"2022-01-04T06:53:06.887905","exception":false,"start_time":"2022-01-04T06:53:06.831048","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.138032Z","iopub.execute_input":"2022-08-21T21:08:50.138507Z","iopub.status.idle":"2022-08-21T21:08:50.15256Z","shell.execute_reply.started":"2022-08-21T21:08:50.13847Z","shell.execute_reply":"2022-08-21T21:08:50.151094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Surface-level information and Shape","metadata":{"papermill":{"duration":0.028907,"end_time":"2022-01-04T06:53:06.945846","exception":false,"start_time":"2022-01-04T06:53:06.916939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print out the head of the dataframe and the shape\ndf.head()","metadata":{"papermill":{"duration":0.057549,"end_time":"2022-01-04T06:53:07.034162","exception":false,"start_time":"2022-01-04T06:53:06.976613","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.15481Z","iopub.execute_input":"2022-08-21T21:08:50.155875Z","iopub.status.idle":"2022-08-21T21:08:50.173106Z","shell.execute_reply.started":"2022-08-21T21:08:50.155833Z","shell.execute_reply":"2022-08-21T21:08:50.171893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# get the shape\ndf.shape","metadata":{"papermill":{"duration":0.039744,"end_time":"2022-01-04T06:53:07.103956","exception":false,"start_time":"2022-01-04T06:53:07.064212","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.174645Z","iopub.execute_input":"2022-08-21T21:08:50.177261Z","iopub.status.idle":"2022-08-21T21:08:50.184371Z","shell.execute_reply.started":"2022-08-21T21:08:50.177208Z","shell.execute_reply":"2022-08-21T21:08:50.183407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: We have 2000 rows/samples and 9 columns. 8 of the 9 columns are our *feature* columns, while the last column (Outcome) represents our *target* column. Moreover, all of our columns appear to consist of numeric data. Let's look at how much of each class we have:","metadata":{"papermill":{"duration":0.029757,"end_time":"2022-01-04T06:53:07.163768","exception":false,"start_time":"2022-01-04T06:53:07.134011","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df[\"Outcome\"].value_counts()","metadata":{"papermill":{"duration":0.048328,"end_time":"2022-01-04T06:53:07.242717","exception":false,"start_time":"2022-01-04T06:53:07.194389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.185472Z","iopub.execute_input":"2022-08-21T21:08:50.186196Z","iopub.status.idle":"2022-08-21T21:08:50.197902Z","shell.execute_reply.started":"2022-08-21T21:08:50.186154Z","shell.execute_reply":"2022-08-21T21:08:50.197043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nsns.countplot(df[\"Outcome\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:08:50.200235Z","iopub.execute_input":"2022-08-21T21:08:50.200915Z","iopub.status.idle":"2022-08-21T21:08:50.364701Z","shell.execute_reply.started":"2022-08-21T21:08:50.200882Z","shell.execute_reply":"2022-08-21T21:08:50.363507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: Our data set consists of 1316 healthy individuals and only 684 diabetic individuals; evidently, our data set is imbalanced. We will need to deal with this later.","metadata":{"papermill":{"duration":0.030294,"end_time":"2022-01-04T06:53:07.304374","exception":false,"start_time":"2022-01-04T06:53:07.27408","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Checking for Null and Duplicate Entries","metadata":{"papermill":{"duration":0.032037,"end_time":"2022-01-04T06:53:07.366823","exception":false,"start_time":"2022-01-04T06:53:07.334786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# check for null entries\ndf.isna().sum()","metadata":{"papermill":{"duration":0.045067,"end_time":"2022-01-04T06:53:07.442542","exception":false,"start_time":"2022-01-04T06:53:07.397475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.366051Z","iopub.execute_input":"2022-08-21T21:08:50.366386Z","iopub.status.idle":"2022-08-21T21:08:50.375776Z","shell.execute_reply.started":"2022-08-21T21:08:50.366356Z","shell.execute_reply":"2022-08-21T21:08:50.374819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for duplicate entries\nnum_duplicate_entries = df.duplicated(subset=None, keep='first').sum()\nnum_duplicate_entries","metadata":{"papermill":{"duration":0.050108,"end_time":"2022-01-04T06:53:07.523877","exception":false,"start_time":"2022-01-04T06:53:07.473769","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.377158Z","iopub.execute_input":"2022-08-21T21:08:50.377875Z","iopub.status.idle":"2022-08-21T21:08:50.390799Z","shell.execute_reply.started":"2022-08-21T21:08:50.377841Z","shell.execute_reply":"2022-08-21T21:08:50.389681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to which class do the duplicates belong to\nduplicate_data = df[df.duplicated(subset=None, keep='first')]\nduplicate_data.Outcome.value_counts()","metadata":{"papermill":{"duration":0.050487,"end_time":"2022-01-04T06:53:07.60525","exception":false,"start_time":"2022-01-04T06:53:07.554763","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.391968Z","iopub.execute_input":"2022-08-21T21:08:50.392827Z","iopub.status.idle":"2022-08-21T21:08:50.40369Z","shell.execute_reply.started":"2022-08-21T21:08:50.392784Z","shell.execute_reply":"2022-08-21T21:08:50.402639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: We don't have any null/missing values, however, we do have 1256 duplicate values--825 of which belong to the healthy class, and 431 of which belong to the diabetic class.","metadata":{"papermill":{"duration":0.030969,"end_time":"2022-01-04T06:53:07.66749","exception":false,"start_time":"2022-01-04T06:53:07.636521","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Feature Analysis\nNow that we have a decent understanding of the structure of our data, let's dive deeper by exploring the features themselves and how they might impact our target variable. To start, we can take a look at the correlation of the features:","metadata":{"papermill":{"duration":0.032282,"end_time":"2022-01-04T06:53:07.730354","exception":false,"start_time":"2022-01-04T06:53:07.698072","status":"completed"},"tags":[]}},{"cell_type":"code","source":"corr = df.corr() # compute the correlation matrix\nmask = np.triu(np.ones_like(corr, dtype=bool)) # define the upper-triangular mask for the heatmap\ncmap = sns.color_palette(\"Blues\", as_cmap=True) # define the color palette to use\nplt.figure(figsize=(12, 10)) # update the figure size to dispaly nicely\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, annot=True)","metadata":{"papermill":{"duration":0.722911,"end_time":"2022-01-04T06:53:08.484512","exception":false,"start_time":"2022-01-04T06:53:07.761601","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.405354Z","iopub.execute_input":"2022-08-21T21:08:50.405728Z","iopub.status.idle":"2022-08-21T21:08:50.953318Z","shell.execute_reply.started":"2022-08-21T21:08:50.405698Z","shell.execute_reply":"2022-08-21T21:08:50.952163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: The features neither have a strong correlation with one another nor the target variable. Thus, we should be able to incorporate all of them when building our final model. Let's take a closer look at their correlations and distributions.","metadata":{"papermill":{"duration":0.033022,"end_time":"2022-01-04T06:53:08.550479","exception":false,"start_time":"2022-01-04T06:53:08.517457","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# use pairplot to show relationships between features and individual distributions\nplt.figure(figsize=(12, 10))\nsns.pairplot(data=df, hue=\"Outcome\", corner=True, diag_kind=\"kde\")","metadata":{"papermill":{"duration":17.692525,"end_time":"2022-01-04T06:53:26.276048","exception":false,"start_time":"2022-01-04T06:53:08.583523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:08:50.954949Z","iopub.execute_input":"2022-08-21T21:08:50.955289Z","iopub.status.idle":"2022-08-21T21:09:03.376492Z","shell.execute_reply.started":"2022-08-21T21:08:50.955257Z","shell.execute_reply":"2022-08-21T21:09:03.375528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: From the scatter plots, we can see that outliers are prevalent within the data set. Moreover, from the KDE distributions on the diagonal, there is a great deal of overlap between healthy and diabetic patients. In other words, there is no subset of features that easily discern healthy individuals from diabetic individuals.","metadata":{"papermill":{"duration":0.047261,"end_time":"2022-01-04T06:53:26.370703","exception":false,"start_time":"2022-01-04T06:53:26.323442","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Outlier Detection\nLet's confirm our assumption that outliers exist in the data set by taking a quick glance at the distribution of the data using panda's `describe` method:","metadata":{"papermill":{"duration":0.046677,"end_time":"2022-01-04T06:53:26.463816","exception":false,"start_time":"2022-01-04T06:53:26.417139","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.describe().T","metadata":{"papermill":{"duration":0.093634,"end_time":"2022-01-04T06:53:26.604006","exception":false,"start_time":"2022-01-04T06:53:26.510372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:03.380384Z","iopub.execute_input":"2022-08-21T21:09:03.380812Z","iopub.status.idle":"2022-08-21T21:09:03.424042Z","shell.execute_reply.started":"2022-08-21T21:09:03.380774Z","shell.execute_reply":"2022-08-21T21:09:03.423052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: In the Glucose, BloodPressure, and BMI columns, there is a large gap from the minimum value to the 25th percentile. Moreover, in many of these columns, the minimum value is zero, which makes little sense in the context of the task (for example, a BloodPressure value of zero indicates that the patient's heart is no longer beating!). In a similar matter, In the Pregnancies, BloodPressure, SkinThickness, Insulin, BMI, and Age columns, there is a large gap from between the 75th percentile to the maximum value, indicating the presence of outliers.","metadata":{"papermill":{"duration":0.047425,"end_time":"2022-01-04T06:53:26.699247","exception":false,"start_time":"2022-01-04T06:53:26.651822","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"To get a sense for just how many outliers we are dealing with, we can use a box and whisker plot over each of the features:","metadata":{"papermill":{"duration":0.047032,"end_time":"2022-01-04T06:53:26.795491","exception":false,"start_time":"2022-01-04T06:53:26.748459","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for column in df.columns[:-1]:\n    plt.figure(figsize=(12, 10))\n    plt.title(f\"{column} Boxplot\")\n    sns.boxplot(data=df, x=column)","metadata":{"papermill":{"duration":2.006015,"end_time":"2022-01-04T06:53:28.849365","exception":false,"start_time":"2022-01-04T06:53:26.84335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:03.425643Z","iopub.execute_input":"2022-08-21T21:09:03.42634Z","iopub.status.idle":"2022-08-21T21:09:05.010757Z","shell.execute_reply.started":"2022-08-21T21:09:03.426285Z","shell.execute_reply":"2022-08-21T21:09:05.009561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make things even more concrete, we can compute the number of outliers present in each of the columns of our data set:","metadata":{"papermill":{"duration":0.053299,"end_time":"2022-01-04T06:53:28.95685","exception":false,"start_time":"2022-01-04T06:53:28.903551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for column in df.columns[:-1]:\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    outlier_range = (df[column] < (Q1 - 1.5*IQR)) | (df[column] > (Q3 + 1.5 * IQR))\n    num_outliers = df[column][outlier_range].count()\n    \n    print(f\"{column}: {num_outliers} outliers\")","metadata":{"papermill":{"duration":0.09589,"end_time":"2022-01-04T06:53:29.106649","exception":false,"start_time":"2022-01-04T06:53:29.010759","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:05.012468Z","iopub.execute_input":"2022-08-21T21:09:05.012806Z","iopub.status.idle":"2022-08-21T21:09:05.039687Z","shell.execute_reply.started":"2022-08-21T21:09:05.012775Z","shell.execute_reply":"2022-08-21T21:09:05.03825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: Looks like our data set contains a large number of outliers; we will need to deal with this later.","metadata":{"papermill":{"duration":0.05367,"end_time":"2022-01-04T06:53:29.214439","exception":false,"start_time":"2022-01-04T06:53:29.160769","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#  Data Preprocessing\nNow that we a have a solid understanding of our data, we can begin to clean and prepare it for our models. Because all of our data is already numeric and no two features have a high correlation, there is not much left for us to do, other than address our class imbalance and standardize our data. ","metadata":{"papermill":{"duration":0.055006,"end_time":"2022-01-04T06:53:29.323809","exception":false,"start_time":"2022-01-04T06:53:29.268803","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Splitting our Data\nWe begin by splitting our data into train and test sets; we must do this *before* any preprocessing so as to avoid any type of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).","metadata":{"papermill":{"duration":0.053368,"end_time":"2022-01-04T06:53:29.432708","exception":false,"start_time":"2022-01-04T06:53:29.37934","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# split our data into train/test sets to avoid data leakage\ntrain_df, test_df = train_test_split(df, test_size=0.25, random_state=0)","metadata":{"papermill":{"duration":0.067028,"end_time":"2022-01-04T06:53:29.555156","exception":false,"start_time":"2022-01-04T06:53:29.488128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:05.04159Z","iopub.execute_input":"2022-08-21T21:09:05.041982Z","iopub.status.idle":"2022-08-21T21:09:05.049709Z","shell.execute_reply.started":"2022-08-21T21:09:05.041949Z","shell.execute_reply":"2022-08-21T21:09:05.048699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Addressing Class Imbalance\nAs we saw earlier, our data is severely imbalanced; we have a greater number of healthy individuals in our data set than diabetic individuals. If this is still the case in our training set, we will perform [random undersampling](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/#:~:text=Random%20undersampling%20involves%20randomly%20selecting,more%20balanced%20distribution%20is%20reached.) to balance our training set. One way we can achieve this is by removing all duplicate entries that were classified as healthy individuals from our training set.","metadata":{"papermill":{"duration":0.054998,"end_time":"2022-01-04T06:53:29.66466","exception":false,"start_time":"2022-01-04T06:53:29.609662","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df[\"Outcome\"].value_counts()","metadata":{"papermill":{"duration":0.069109,"end_time":"2022-01-04T06:53:29.787957","exception":false,"start_time":"2022-01-04T06:53:29.718848","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:05.050772Z","iopub.execute_input":"2022-08-21T21:09:05.051141Z","iopub.status.idle":"2022-08-21T21:09:05.065844Z","shell.execute_reply.started":"2022-08-21T21:09:05.051111Z","shell.execute_reply":"2022-08-21T21:09:05.064621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evidently, our training set is still heavily imbalanced. Hence, random undersampling needs to be done. One way we can achieve this is by removing all the duplicate entries that were classified as healthy individuals from our training set.","metadata":{"papermill":{"duration":0.053676,"end_time":"2022-01-04T06:53:29.89526","exception":false,"start_time":"2022-01-04T06:53:29.841584","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get all the duplicated rows\ntrain_dups = train_df[train_df.duplicated(subset=None, keep='first')]\n# get the index of all the healthy duplicates\nhealthy_dups = train_dups.loc[train_df[\"Outcome\"] == 0].index\n# drop the healthy duplicates\ntrain_df = train_df.drop(healthy_dups)","metadata":{"papermill":{"duration":0.072605,"end_time":"2022-01-04T06:53:30.022932","exception":false,"start_time":"2022-01-04T06:53:29.950327","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:05.067359Z","iopub.execute_input":"2022-08-21T21:09:05.068024Z","iopub.status.idle":"2022-08-21T21:09:05.082356Z","shell.execute_reply.started":"2022-08-21T21:09:05.067985Z","shell.execute_reply":"2022-08-21T21:09:05.080695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Outcome\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:05.083554Z","iopub.execute_input":"2022-08-21T21:09:05.084773Z","iopub.status.idle":"2022-08-21T21:09:05.094921Z","shell.execute_reply.started":"2022-08-21T21:09:05.084708Z","shell.execute_reply":"2022-08-21T21:09:05.093954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've performed random sampling, we can standardize our data. Because we have a large number of outliers, we will be using sklearn's [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).","metadata":{"papermill":{"duration":0.054357,"end_time":"2022-01-04T06:53:30.132545","exception":false,"start_time":"2022-01-04T06:53:30.078188","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# separate the data from the labels\nX_train, y_train = train_df.drop(columns=[\"Outcome\"], axis=1), train_df[\"Outcome\"]\nX_test, y_test = test_df.drop(columns=[\"Outcome\"], axis=1), test_df[\"Outcome\"]\n\n# create the scaler\nscaler = RobustScaler()\n# fit the scaler and transform our data\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"papermill":{"duration":0.077856,"end_time":"2022-01-04T06:53:30.264222","exception":false,"start_time":"2022-01-04T06:53:30.186366","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-21T21:09:05.095991Z","iopub.execute_input":"2022-08-21T21:09:05.09657Z","iopub.status.idle":"2022-08-21T21:09:05.116383Z","shell.execute_reply.started":"2022-08-21T21:09:05.096538Z","shell.execute_reply":"2022-08-21T21:09:05.115529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Predictions and Evaluation\nNow all that we have left to do is generate some models and make predictions!","metadata":{"papermill":{"duration":0.054168,"end_time":"2022-01-04T06:53:30.373054","exception":false,"start_time":"2022-01-04T06:53:30.318886","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\ndef model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:05.117976Z","iopub.execute_input":"2022-08-21T21:09:05.118696Z","iopub.status.idle":"2022-08-21T21:09:05.129373Z","shell.execute_reply.started":"2022-08-21T21:09:05.118654Z","shell.execute_reply":"2022-08-21T21:09:05.127997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomForest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nacc_rf= model_Evaluate(rf)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:05.146023Z","iopub.execute_input":"2022-08-21T21:09:05.146604Z","iopub.status.idle":"2022-08-21T21:09:05.607898Z","shell.execute_reply.started":"2022-08-21T21:09:05.146571Z","shell.execute_reply":"2022-08-21T21:09:05.606777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"dc = DecisionTreeClassifier()\ndc.fit(X_train, y_train)\nacc_dc= model_Evaluate(dc)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:05.609429Z","iopub.execute_input":"2022-08-21T21:09:05.610124Z","iopub.status.idle":"2022-08-21T21:09:05.823694Z","shell.execute_reply.started":"2022-08-21T21:09:05.610076Z","shell.execute_reply":"2022-08-21T21:09:05.822713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNeighborsClassifier","metadata":{}},{"cell_type":"code","source":"kn = KNeighborsClassifier()\nkn.fit(X_train, y_train)\nacc_kn= model_Evaluate(kn)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:05.825104Z","iopub.execute_input":"2022-08-21T21:09:05.826171Z","iopub.status.idle":"2022-08-21T21:09:06.096005Z","shell.execute_reply.started":"2022-08-21T21:09:05.826132Z","shell.execute_reply":"2022-08-21T21:09:06.0947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\nacc_lr= model_Evaluate(lr)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:06.097707Z","iopub.execute_input":"2022-08-21T21:09:06.098075Z","iopub.status.idle":"2022-08-21T21:09:06.368202Z","shell.execute_reply.started":"2022-08-21T21:09:06.098044Z","shell.execute_reply":"2022-08-21T21:09:06.367102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\nacc_xg= model_Evaluate(xg)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:06.369656Z","iopub.execute_input":"2022-08-21T21:09:06.369985Z","iopub.status.idle":"2022-08-21T21:09:07.087632Z","shell.execute_reply.started":"2022-08-21T21:09:06.369955Z","shell.execute_reply":"2022-08-21T21:09:07.086393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROC curve","metadata":{}},{"cell_type":"code","source":"model_lr = LogisticRegression().fit(X_train, y_train)\nprobs_lr = model_lr.predict_proba(X_test)[:, 1]\n\nmodel_dt = DecisionTreeClassifier().fit(X_train, y_train)\nprobs_dt = model_dt.predict_proba(X_test)[:, 1]\n\nmodel_kn = KNeighborsClassifier().fit(X_train, y_train)\nprobs_kn = model_kn.predict_proba(X_test)[:, 1]\n\nmodel_rf = RandomForestClassifier().fit(X_train, y_train)\nprobs_rf = model_rf.predict_proba(X_test)[:, 1]\n\nmodel_xg = XGBClassifier().fit(X_train, y_train)\nprobs_xg = model_xg.predict_proba(X_test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:07.089459Z","iopub.execute_input":"2022-08-21T21:09:07.090289Z","iopub.status.idle":"2022-08-21T21:09:07.812734Z","shell.execute_reply.started":"2022-08-21T21:09:07.090243Z","shell.execute_reply":"2022-08-21T21:09:07.811712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve\n\ny_test_int = y_test.replace({'Good': 1, 'Bad': 0})\nauc_lr = roc_auc_score(y_test_int, probs_lr)\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_int, probs_lr)\n\nauc_dt = roc_auc_score(y_test_int, probs_dt)\nfpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test_int, probs_dt)\n\nauc_kn = roc_auc_score(y_test_int, probs_kn)\nfpr_kn, tpr_kn, thresholds_kn = roc_curve(y_test_int, probs_kn)\n\nauc_rf = roc_auc_score(y_test_int, probs_rf)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test_int, probs_rf)\n\nauc_xg = roc_auc_score(y_test_int, probs_xg)\nfpr_xg, tpr_xg, thresholds_xg = roc_curve(y_test_int, probs_xg)\n\nplt.figure(figsize=(12, 7))\nplt.plot(fpr_lr, tpr_lr, label=f'AUC (Logistic Regression) = {auc_lr:.2f}')\nplt.plot(fpr_dt, tpr_dt, label=f'AUC (Decision Tree) = {auc_dt:.2f}')\nplt.plot(fpr_kn, tpr_kn, label=f'AUC (K-nearest Neighbors) = {auc_kn:.2f}')\nplt.plot(fpr_rf, tpr_rf, label=f'AUC (Random Forests) = {auc_rf:.2f}')\nplt.plot(fpr_xg, tpr_xg, label=f'AUC (XGBoost) = {auc_xg:.2f}')\nplt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\nplt.title('ROC Curve', size=20)\nplt.xlabel('False Positive Rate', size=14)\nplt.ylabel('True Positive Rate', size=14)\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-08-21T21:09:07.814299Z","iopub.execute_input":"2022-08-21T21:09:07.815297Z","iopub.status.idle":"2022-08-21T21:09:08.157443Z","shell.execute_reply.started":"2022-08-21T21:09:07.815257Z","shell.execute_reply":"2022-08-21T21:09:08.15624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thanks to https://www.kaggle.com/code/anandmural/diabetesprediction#Diabetes-Prediction for the insight...**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}